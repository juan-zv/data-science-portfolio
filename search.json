[
  {
    "objectID": "Cleansing_Projects/project4.html",
    "href": "Cleansing_Projects/project4.html",
    "title": "Client Report - Can you predict that?",
    "section": "",
    "text": "We can predict if a house was built before or after 1980 with the correct training model. I also show the accuracyscore of the model I chose for this case. It is really interesting to see how sklearn provides tools to train and test ML models.\n\n\nRead and format project data\n# Include and execute your code here\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\n\ndwellings_ml = pd.read_csv(url)\n\ndwellings_ml.head()\n\n\n\n\n\n\n\n\n\nparcel\nabstrprd\nlivearea\nfinbsmnt\nbasement\nyrbuilt\ntotunits\nstories\nnocars\nnumbdrm\n...\narcstyle_THREE-STORY\narcstyle_TRI-LEVEL\narcstyle_TRI-LEVEL WITH BASEMENT\narcstyle_TWO AND HALF-STORY\narcstyle_TWO-STORY\nqualified_Q\nqualified_U\nstatus_I\nstatus_V\nbefore1980\n\n\n\n\n0\n00102-08-065-065\n1130\n1346\n0\n0\n2004\n1\n2\n2\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n1\n00102-08-073-073\n1130\n1249\n0\n0\n2005\n1\n1\n1\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n2\n00102-08-078-078\n1130\n1346\n0\n0\n2005\n1\n2\n1\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n3\n00102-08-081-081\n1130\n1146\n0\n0\n2005\n1\n1\n0\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n4\n00102-08-086-086\n1130\n1249\n0\n0\n2005\n1\n1\n1\n2\n...\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n\n\n\n\n5 rows × 51 columns"
  },
  {
    "objectID": "Cleansing_Projects/project4.html#elevator-pitch",
    "href": "Cleansing_Projects/project4.html#elevator-pitch",
    "title": "Client Report - Can you predict that?",
    "section": "",
    "text": "We can predict if a house was built before or after 1980 with the correct training model. I also show the accuracyscore of the model I chose for this case. It is really interesting to see how sklearn provides tools to train and test ML models.\n\n\nRead and format project data\n# Include and execute your code here\nurl = \"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\"\n\ndwellings_ml = pd.read_csv(url)\n\ndwellings_ml.head()\n\n\n\n\n\n\n\n\n\nparcel\nabstrprd\nlivearea\nfinbsmnt\nbasement\nyrbuilt\ntotunits\nstories\nnocars\nnumbdrm\n...\narcstyle_THREE-STORY\narcstyle_TRI-LEVEL\narcstyle_TRI-LEVEL WITH BASEMENT\narcstyle_TWO AND HALF-STORY\narcstyle_TWO-STORY\nqualified_Q\nqualified_U\nstatus_I\nstatus_V\nbefore1980\n\n\n\n\n0\n00102-08-065-065\n1130\n1346\n0\n0\n2004\n1\n2\n2\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n1\n00102-08-073-073\n1130\n1249\n0\n0\n2005\n1\n1\n1\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n2\n00102-08-078-078\n1130\n1346\n0\n0\n2005\n1\n2\n1\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n3\n00102-08-081-081\n1130\n1146\n0\n0\n2005\n1\n1\n0\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n4\n00102-08-086-086\n1130\n1249\n0\n0\n2005\n1\n1\n1\n2\n...\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n\n\n\n\n5 rows × 51 columns"
  },
  {
    "objectID": "Cleansing_Projects/project4.html#questiontask-1",
    "href": "Cleansing_Projects/project4.html#questiontask-1",
    "title": "Client Report - Can you predict that?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\nI chose to display3 charts comparing the relationship between the Year Built and the Sell Price, Number of Bathroms, and Number of Bedrooms. My assumption is that these comparissons can help us understand what data could be used to train the model and how data is shaped along the years.\n\n\nShow the code\n# Relationship between the year built and the sell prices\nfig1 = px.scatter(dwellings_ml, x='yrbuilt', y='sprice', color='sprice', title='Sell Price and Year Built', labels={'yrbuilt': 'Year Built', 'sprice': 'Sell Price'\n})\n\n\n# Relationship between the year built and the number of bathrooms\nbaths_count = dwellings_ml.groupby(['yrbuilt', 'numbaths']).size().reset_index(name='count')\n\nfig2 = px.bar(baths_count, x='yrbuilt', y='count', color='numbaths', title='Number of Bathrooms vs. Year Built',\n              labels={'yrbuilt': 'Year Built', 'count': 'Number of Houses', 'numbaths': 'Number of Bathrooms'})\n\n\n# Relationship between the year built and the number of bedrooms\nbedrooms_count = dwellings_ml.groupby(['yrbuilt', 'numbdrm']).size().reset_index(name='count')\n\nfig3 = px.bar(bedrooms_count, x='yrbuilt', y='count', color='numbdrm', title='Number of Bedrooms vs. Year Built',\n              labels={'yrbuilt': 'Year Built', 'count': 'Number of Houses', 'numbdrm': 'Number of Bedrooms'})\n\n\nfig1.show()\nfig2.show()\nfig3.show()"
  },
  {
    "objectID": "Cleansing_Projects/project4.html#questiontask-2",
    "href": "Cleansing_Projects/project4.html#questiontask-2",
    "title": "Client Report - Can you predict that?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\nI chose to use a Decision Tree Classifier, and a Random Forest Classifier which was pretty good at guessing the yearbuilt. I dropped columns which are not related to the yearbuilt because are random like parcel, or ones that are too crucial for the model like yrbuilt. After comparing the accuracy result, I decided that a Random Forest Classifier would work best.\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Columns to drop to prepare training and test data\nfeatures_to_drop = ['parcel', 'abstrprd', 'before1980', 'yrbuilt']\nX = dwellings_ml.drop(columns=features_to_drop)\ny = dwellings_ml['before1980']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Show the shape of the datasets to confirm the split\n# (X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\ndt_classifier = DecisionTreeClassifier(random_state=42)\n\ndt_classifier.fit(X_train, y_train)\n\ny_pred = dt_classifier.predict(X_test)\n\n# Calculate accuracy\naccuracy_score(y_test, y_pred)\n\n\n0.9039860343322665\n\n\n\n\nShow the code\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize the Random Forest Classifier\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Train the model on the training data\nrf_classifier.fit(X_train, y_train)\n\n# Predict on the testing data\ny_pred_rf = rf_classifier.predict(X_test)\n\naccuracy_score(y_test, y_pred_rf)\n\n\n0.9262438172825138"
  },
  {
    "objectID": "Cleansing_Projects/project4.html#questiontask-3",
    "href": "Cleansing_Projects/project4.html#questiontask-3",
    "title": "Client Report - Can you predict that?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.\nThe most important features selected by the RFC are the are which hauses belong to, the style of architecture like having one story and the number of bathrooms. These show that mostly these columns where used to predict data. This helped the model to be 92% accurate.\n\n\nShow the code\n# Extract feature importances from the model\nfeature_importances = rf_classifier.feature_importances_\n\n# Create a DataFrame for visualization\nfeatures_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n\n# Sort the DataFrame by importance\nfeatures_df = features_df.sort_values(by='Importance', ascending=False)\n\n# Visualizing the most important features\nfig = px.bar(features_df.head(5), x='Importance', y='Feature', orientation='h',\n             title='5 Most Important Features in Predicting Year Built',\n             labels={'Feature': 'Feature', 'Importance': 'Importance Score'})\n\nfig.show()"
  },
  {
    "objectID": "Cleansing_Projects/project4.html#questiontask-4",
    "href": "Cleansing_Projects/project4.html#questiontask-4",
    "title": "Client Report - Can you predict that?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\nI chose the Precision, Recall and AUROC score. These scores show how good a model is by giving a score on how often the model is right at predicting, how good the model is at predicting a ‘before 1980’ house, and how the model overall is able to predict a house built before or after 1980; respectively.\n\n\nShow the code\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score\n\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nauroc = roc_auc_score(y_test, y_pred)\n\n(precision, recall, auroc)\n\n\n(0.9249242247610165, 0.9214866434378629, 0.898073021991411)"
  },
  {
    "objectID": "Cleansing_Projects/project0.html",
    "href": "Cleansing_Projects/project0.html",
    "title": "Client Report - Project 0",
    "section": "",
    "text": "Working with csv files is common in data analysis because it allows to store data with a simple format and to use it with data analysis like Pandas and Plotly Express. Plotly Express let us graph data to visualize results and better analyze\n\n\nRead and format project data\nmpg = pd.read_csv(\"https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/mpg/mpg.csv\")"
  },
  {
    "objectID": "Cleansing_Projects/project0.html#elevator-pitch",
    "href": "Cleansing_Projects/project0.html#elevator-pitch",
    "title": "Client Report - Project 0",
    "section": "",
    "text": "Working with csv files is common in data analysis because it allows to store data with a simple format and to use it with data analysis like Pandas and Plotly Express. Plotly Express let us graph data to visualize results and better analyze\n\n\nRead and format project data\nmpg = pd.read_csv(\"https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/mpg/mpg.csv\")"
  },
  {
    "objectID": "Cleansing_Projects/project0.html#questiontask-1",
    "href": "Cleansing_Projects/project0.html#questiontask-1",
    "title": "Client Report - Project 0",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite a python script to create the example chart from section 3.2.2 of the textbook (part of the assigned readings).\n\n\nRead and format data\nfig = px.scatter(mpg, x=\"displ\", y=\"hwy\", title=\"MPG\")\n\nfig.update_layout(xaxis_title=\"Car's engine size in liters\", yaxis_title=\"Car's fuel efficiency\")\n\nfig.show()\n\n(mpg\n  .head(5)\n  .filter([\"manufacturer\", \"model\",\"year\", \"hwy\"])\n)\n\n\n                                                \n\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\nyear\nhwy\n\n\n\n\n0\naudi\na4\n1999\n29\n\n\n1\naudi\na4\n1999\n29\n\n\n2\naudi\na4\n2008\n31\n\n\n3\naudi\na4\n2008\n30\n\n\n4\naudi\na4\n1999\n26"
  },
  {
    "objectID": "Cleansing_Projects/project5.html",
    "href": "Cleansing_Projects/project5.html",
    "title": "Client Report - The war with the Star Wars",
    "section": "",
    "text": "It is really interesting to build a machine learning model which can predict if a person makes more or less than $ 50K using data provided from a survey about Star Wars. Yes, a sruvey about Star Wars could technically predict this. This is really interesting, and I think really cool to try at least.\n\n\nRead and format project data\ndf = pd.read_csv('https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv', encoding='ISO-8859-1')"
  },
  {
    "objectID": "Cleansing_Projects/project5.html#elevator-pitch",
    "href": "Cleansing_Projects/project5.html#elevator-pitch",
    "title": "Client Report - The war with the Star Wars",
    "section": "",
    "text": "It is really interesting to build a machine learning model which can predict if a person makes more or less than $ 50K using data provided from a survey about Star Wars. Yes, a sruvey about Star Wars could technically predict this. This is really interesting, and I think really cool to try at least.\n\n\nRead and format project data\ndf = pd.read_csv('https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv', encoding='ISO-8859-1')"
  },
  {
    "objectID": "Cleansing_Projects/project5.html#questiontask-1",
    "href": "Cleansing_Projects/project5.html#questiontask-1",
    "title": "Client Report - The war with the Star Wars",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\n\n\nShow the code\ncolumn_mapping = {\n    'RespondentID': 'RespondentID',\n    'Have you seen any of the 6 films in the Star Wars franchise?': 'SeenAnyFilm',\n    'Do you consider yourself to be a fan of the Star Wars film franchise?': 'FanOfFranchise',\n    'Which of the following Star Wars films have you seen? Please select all that apply.': 'SeenFilm1',\n    'Unnamed: 4': 'SeenFilm2',\n    'Unnamed: 5': 'SeenFilm3',\n    'Unnamed: 6': 'SeenFilm4',\n    'Unnamed: 7': 'SeenFilm5',\n    'Unnamed: 8': 'SeenFilm6',\n    'Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.': 'RankFilm1',\n    'Unnamed: 10': 'RankFilm2',\n    'Unnamed: 11': 'RankFilm3',\n    'Unnamed: 12': 'RankFilm4',\n    'Unnamed: 13': 'RankFilm5',\n    'Unnamed: 14': 'RankFilm6',\n    'Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.': 'HanSoloOpinion',\n    'Unnamed: 16': 'LukeSkywalkerOpinion',\n    'Unnamed: 17': 'PrincessLeiaOrganaOpinion',\n    'Unnamed: 18': 'AnakinSkywalkerOpinion',\n    'Unnamed: 19': 'ObiWanKenobiOpinion',\n    'Unnamed: 20': 'EmperorPalpatineOpinion',\n    'Unnamed: 21': 'DarthVaderOpinion',\n    'Unnamed: 22': 'LandoCalrissianOpinion',\n    'Unnamed: 23': 'BobaFettOpinion',\n    'Unnamed: 24': 'C-3P0Opinion',\n    'Unnamed: 25': 'R2D2Opinion',\n    'Unnamed: 26': 'JarJarBinksOpinion',\n    'Unnamed: 27': 'PadmeAmidalaOpinion',\n    'Unnamed: 28': 'YodaOpinion',\n    'Which character shot first?': 'WhoShotFirst',\n    'Are you familiar with the Expanded Universe?': 'KnowsExpandedUniverse',\n    'Do you consider yourself to be a fan of the Expanded Universe?æ': 'FanOfExpandedUniverse',\n    'Do you consider yourself to be a fan of the Star Trek franchise?': 'FanOfStarTrek',\n    'Household Income': 'Income',\n    'Location (Census Region)': 'Location'\n}\n\ndf_cleaned = df.rename(columns=column_mapping)\n\ndf_cleaned.columns\n\n\nIndex(['RespondentID', 'SeenAnyFilm', 'FanOfFranchise', 'SeenFilm1',\n       'SeenFilm2', 'SeenFilm3', 'SeenFilm4', 'SeenFilm5', 'SeenFilm6',\n       'RankFilm1', 'RankFilm2', 'RankFilm3', 'RankFilm4', 'RankFilm5',\n       'RankFilm6', 'HanSoloOpinion', 'LukeSkywalkerOpinion',\n       'PrincessLeiaOrganaOpinion', 'AnakinSkywalkerOpinion',\n       'ObiWanKenobiOpinion', 'EmperorPalpatineOpinion', 'DarthVaderOpinion',\n       'LandoCalrissianOpinion', 'BobaFettOpinion', 'C-3P0Opinion',\n       'R2D2Opinion', 'JarJarBinksOpinion', 'PadmeAmidalaOpinion',\n       'YodaOpinion', 'WhoShotFirst', 'KnowsExpandedUniverse',\n       'FanOfExpandedUniverse', 'FanOfStarTrek', 'Gender', 'Age', 'Income',\n       'Education', 'Location'],\n      dtype='object')"
  },
  {
    "objectID": "Cleansing_Projects/project5.html#questiontask-2",
    "href": "Cleansing_Projects/project5.html#questiontask-2",
    "title": "Client Report - The war with the Star Wars",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made\nFilter the dataset to respondents that have seen at least one film.\n\n\nShow the code\ndf_filtered = df_cleaned.query('SeenAnyFilm == \"Yes\"')\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nRespondentID\nSeenAnyFilm\nFanOfFranchise\nSeenFilm1\nSeenFilm2\nSeenFilm3\nSeenFilm4\nSeenFilm5\nSeenFilm6\nRankFilm1\n...\nYodaOpinion\nWhoShotFirst\nKnowsExpandedUniverse\nFanOfExpandedUniverse\nFanOfStarTrek\nGender\nAge\nIncome\nEducation\nLocation\n\n\n\n\n1\n3.292880e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n3\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\n18-29\nNaN\nHigh school degree\nSouth Atlantic\n\n\n3\n3.292765e+09\nYes\nNo\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nNaN\nNaN\nNaN\n1\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n18-29\n$0 - $24,999\nHigh school degree\nWest North Central\n\n\n4\n3.292763e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n5\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n5\n3.292731e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n5\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n6\n3.292719e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n1\n...\nVery favorably\nHan\nYes\nNo\nYes\nMale\n18-29\n$25,000 - $49,999\nBachelor degree\nMiddle Atlantic\n\n\n\n\n5 rows × 38 columns\n\n\n\nCreate a new column that converts the age ranges to a single number. Drop the age range categorical column.\n\n\nShow the code\nage_mapping = {\n    '18-29': 1,\n    '30-44': 2,\n    '45-60': 3,\n    '&gt; 60': 4\n}\ndf_filtered['AgeGroup'] = df_filtered['Age'].map(age_mapping)\ndf_filtered = df_filtered.drop(columns=['Age'])\n\ndf_filtered.columns\n\n\nIndex(['RespondentID', 'SeenAnyFilm', 'FanOfFranchise', 'SeenFilm1',\n       'SeenFilm2', 'SeenFilm3', 'SeenFilm4', 'SeenFilm5', 'SeenFilm6',\n       'RankFilm1', 'RankFilm2', 'RankFilm3', 'RankFilm4', 'RankFilm5',\n       'RankFilm6', 'HanSoloOpinion', 'LukeSkywalkerOpinion',\n       'PrincessLeiaOrganaOpinion', 'AnakinSkywalkerOpinion',\n       'ObiWanKenobiOpinion', 'EmperorPalpatineOpinion', 'DarthVaderOpinion',\n       'LandoCalrissianOpinion', 'BobaFettOpinion', 'C-3P0Opinion',\n       'R2D2Opinion', 'JarJarBinksOpinion', 'PadmeAmidalaOpinion',\n       'YodaOpinion', 'WhoShotFirst', 'KnowsExpandedUniverse',\n       'FanOfExpandedUniverse', 'FanOfStarTrek', 'Gender', 'Income',\n       'Education', 'Location', 'AgeGroup'],\n      dtype='object')\n\n\nCreate a new column that converts the education groupings to a single number. Drop the school categorical column\n\n\nShow the code\neducation_mapping = {\n    'Less than high school degree': 1,\n    'High school degree': 2,\n    'Some college or Associate degree': 3,\n    'Bachelor degree': 4,\n    'Graduate degree': 5\n}\ndf_filtered['EducationGroup'] = df_filtered['Education'].map(education_mapping)\ndf_filtered = df_filtered.drop(columns=['Education'])\n\ndf_filtered.columns\n\n\nIndex(['RespondentID', 'SeenAnyFilm', 'FanOfFranchise', 'SeenFilm1',\n       'SeenFilm2', 'SeenFilm3', 'SeenFilm4', 'SeenFilm5', 'SeenFilm6',\n       'RankFilm1', 'RankFilm2', 'RankFilm3', 'RankFilm4', 'RankFilm5',\n       'RankFilm6', 'HanSoloOpinion', 'LukeSkywalkerOpinion',\n       'PrincessLeiaOrganaOpinion', 'AnakinSkywalkerOpinion',\n       'ObiWanKenobiOpinion', 'EmperorPalpatineOpinion', 'DarthVaderOpinion',\n       'LandoCalrissianOpinion', 'BobaFettOpinion', 'C-3P0Opinion',\n       'R2D2Opinion', 'JarJarBinksOpinion', 'PadmeAmidalaOpinion',\n       'YodaOpinion', 'WhoShotFirst', 'KnowsExpandedUniverse',\n       'FanOfExpandedUniverse', 'FanOfStarTrek', 'Gender', 'Income',\n       'Location', 'AgeGroup', 'EducationGroup'],\n      dtype='object')\n\n\nCreate a new column that converts the income ranges to a single number. Drop the income range categorical column.\n\n\nShow the code\nincome_mapping = {\n    '$0 - $24,999': 1,\n    '$25,000 - $49,999': 2,\n    '$50,000 - $99,999': 3,\n    '$100,000 - $149,999': 4,\n    '$150,000+': 5\n}\ndf_filtered['IncomeGroup'] = df_filtered['Income'].map(income_mapping)\ndf_filtered = df_filtered.drop(columns=['Income'])\n\ndf_filtered.columns\n\n\nIndex(['RespondentID', 'SeenAnyFilm', 'FanOfFranchise', 'SeenFilm1',\n       'SeenFilm2', 'SeenFilm3', 'SeenFilm4', 'SeenFilm5', 'SeenFilm6',\n       'RankFilm1', 'RankFilm2', 'RankFilm3', 'RankFilm4', 'RankFilm5',\n       'RankFilm6', 'HanSoloOpinion', 'LukeSkywalkerOpinion',\n       'PrincessLeiaOrganaOpinion', 'AnakinSkywalkerOpinion',\n       'ObiWanKenobiOpinion', 'EmperorPalpatineOpinion', 'DarthVaderOpinion',\n       'LandoCalrissianOpinion', 'BobaFettOpinion', 'C-3P0Opinion',\n       'R2D2Opinion', 'JarJarBinksOpinion', 'PadmeAmidalaOpinion',\n       'YodaOpinion', 'WhoShotFirst', 'KnowsExpandedUniverse',\n       'FanOfExpandedUniverse', 'FanOfStarTrek', 'Gender', 'Location',\n       'AgeGroup', 'EducationGroup', 'IncomeGroup'],\n      dtype='object')\n\n\nCreate your target (also known as “y” or “label”) column based on the new income range column.\n\n\nShow the code\ndf_filtered['Target'] = (df_filtered['IncomeGroup'] &gt; 2)\n\ndf_filtered.columns\n\ndf_filtered.head()\n\n\n\n\n\n\n\n\n\nRespondentID\nSeenAnyFilm\nFanOfFranchise\nSeenFilm1\nSeenFilm2\nSeenFilm3\nSeenFilm4\nSeenFilm5\nSeenFilm6\nRankFilm1\n...\nWhoShotFirst\nKnowsExpandedUniverse\nFanOfExpandedUniverse\nFanOfStarTrek\nGender\nLocation\nAgeGroup\nEducationGroup\nIncomeGroup\nTarget\n\n\n\n\n1\n3.292880e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n3\n...\nI don't understand this question\nYes\nNo\nNo\nMale\nSouth Atlantic\n1.0\n2.0\nNaN\nFalse\n\n\n3\n3.292765e+09\nYes\nNo\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nNaN\nNaN\nNaN\n1\n...\nI don't understand this question\nNo\nNaN\nNo\nMale\nWest North Central\n1.0\n2.0\n1.0\nFalse\n\n\n4\n3.292763e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n5\n...\nI don't understand this question\nNo\nNaN\nYes\nMale\nWest North Central\n1.0\n3.0\n4.0\nTrue\n\n\n5\n3.292731e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n5\n...\nGreedo\nYes\nNo\nNo\nMale\nWest North Central\n1.0\n3.0\n4.0\nTrue\n\n\n6\n3.292719e+09\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n1\n...\nHan\nYes\nNo\nYes\nMale\nMiddle Atlantic\n1.0\n4.0\n2.0\nFalse\n\n\n\n\n5 rows × 39 columns\n\n\n\nOne-hot encode all remaining categorical columns.\n::: {#cell-Q2f Get Dummies .cell execution_count=9}\n\nShow the code\ndf_filtered_ohe = pd.get_dummies(df_filtered)\ndf_filtered_ohe.columns.tolist()\n\n\n['RespondentID',\n 'AgeGroup',\n 'EducationGroup',\n 'IncomeGroup',\n 'Target',\n 'SeenAnyFilm_Yes',\n 'FanOfFranchise_No',\n 'FanOfFranchise_Yes',\n 'SeenFilm1_Star Wars: Episode I  The Phantom Menace',\n 'SeenFilm2_Star Wars: Episode II  Attack of the Clones',\n 'SeenFilm3_Star Wars: Episode III  Revenge of the Sith',\n 'SeenFilm4_Star Wars: Episode IV  A New Hope',\n 'SeenFilm5_Star Wars: Episode V The Empire Strikes Back',\n 'SeenFilm6_Star Wars: Episode VI Return of the Jedi',\n 'RankFilm1_1',\n 'RankFilm1_2',\n 'RankFilm1_3',\n 'RankFilm1_4',\n 'RankFilm1_5',\n 'RankFilm1_6',\n 'RankFilm2_1',\n 'RankFilm2_2',\n 'RankFilm2_3',\n 'RankFilm2_4',\n 'RankFilm2_5',\n 'RankFilm2_6',\n 'RankFilm3_1',\n 'RankFilm3_2',\n 'RankFilm3_3',\n 'RankFilm3_4',\n 'RankFilm3_5',\n 'RankFilm3_6',\n 'RankFilm4_1',\n 'RankFilm4_2',\n 'RankFilm4_3',\n 'RankFilm4_4',\n 'RankFilm4_5',\n 'RankFilm4_6',\n 'RankFilm5_1',\n 'RankFilm5_2',\n 'RankFilm5_3',\n 'RankFilm5_4',\n 'RankFilm5_5',\n 'RankFilm5_6',\n 'RankFilm6_1',\n 'RankFilm6_2',\n 'RankFilm6_3',\n 'RankFilm6_4',\n 'RankFilm6_5',\n 'RankFilm6_6',\n 'HanSoloOpinion_Neither favorably nor unfavorably (neutral)',\n 'HanSoloOpinion_Somewhat favorably',\n 'HanSoloOpinion_Somewhat unfavorably',\n 'HanSoloOpinion_Unfamiliar (N/A)',\n 'HanSoloOpinion_Very favorably',\n 'HanSoloOpinion_Very unfavorably',\n 'LukeSkywalkerOpinion_Neither favorably nor unfavorably (neutral)',\n 'LukeSkywalkerOpinion_Somewhat favorably',\n 'LukeSkywalkerOpinion_Somewhat unfavorably',\n 'LukeSkywalkerOpinion_Unfamiliar (N/A)',\n 'LukeSkywalkerOpinion_Very favorably',\n 'LukeSkywalkerOpinion_Very unfavorably',\n 'PrincessLeiaOrganaOpinion_Neither favorably nor unfavorably (neutral)',\n 'PrincessLeiaOrganaOpinion_Somewhat favorably',\n 'PrincessLeiaOrganaOpinion_Somewhat unfavorably',\n 'PrincessLeiaOrganaOpinion_Unfamiliar (N/A)',\n 'PrincessLeiaOrganaOpinion_Very favorably',\n 'PrincessLeiaOrganaOpinion_Very unfavorably',\n 'AnakinSkywalkerOpinion_Neither favorably nor unfavorably (neutral)',\n 'AnakinSkywalkerOpinion_Somewhat favorably',\n 'AnakinSkywalkerOpinion_Somewhat unfavorably',\n 'AnakinSkywalkerOpinion_Unfamiliar (N/A)',\n 'AnakinSkywalkerOpinion_Very favorably',\n 'AnakinSkywalkerOpinion_Very unfavorably',\n 'ObiWanKenobiOpinion_Neither favorably nor unfavorably (neutral)',\n 'ObiWanKenobiOpinion_Somewhat favorably',\n 'ObiWanKenobiOpinion_Somewhat unfavorably',\n 'ObiWanKenobiOpinion_Unfamiliar (N/A)',\n 'ObiWanKenobiOpinion_Very favorably',\n 'ObiWanKenobiOpinion_Very unfavorably',\n 'EmperorPalpatineOpinion_Neither favorably nor unfavorably (neutral)',\n 'EmperorPalpatineOpinion_Somewhat favorably',\n 'EmperorPalpatineOpinion_Somewhat unfavorably',\n 'EmperorPalpatineOpinion_Unfamiliar (N/A)',\n 'EmperorPalpatineOpinion_Very favorably',\n 'EmperorPalpatineOpinion_Very unfavorably',\n 'DarthVaderOpinion_Neither favorably nor unfavorably (neutral)',\n 'DarthVaderOpinion_Somewhat favorably',\n 'DarthVaderOpinion_Somewhat unfavorably',\n 'DarthVaderOpinion_Unfamiliar (N/A)',\n 'DarthVaderOpinion_Very favorably',\n 'DarthVaderOpinion_Very unfavorably',\n 'LandoCalrissianOpinion_Neither favorably nor unfavorably (neutral)',\n 'LandoCalrissianOpinion_Somewhat favorably',\n 'LandoCalrissianOpinion_Somewhat unfavorably',\n 'LandoCalrissianOpinion_Unfamiliar (N/A)',\n 'LandoCalrissianOpinion_Very favorably',\n 'LandoCalrissianOpinion_Very unfavorably',\n 'BobaFettOpinion_Neither favorably nor unfavorably (neutral)',\n 'BobaFettOpinion_Somewhat favorably',\n 'BobaFettOpinion_Somewhat unfavorably',\n 'BobaFettOpinion_Unfamiliar (N/A)',\n 'BobaFettOpinion_Very favorably',\n 'BobaFettOpinion_Very unfavorably',\n 'C-3P0Opinion_Neither favorably nor unfavorably (neutral)',\n 'C-3P0Opinion_Somewhat favorably',\n 'C-3P0Opinion_Somewhat unfavorably',\n 'C-3P0Opinion_Unfamiliar (N/A)',\n 'C-3P0Opinion_Very favorably',\n 'C-3P0Opinion_Very unfavorably',\n 'R2D2Opinion_Neither favorably nor unfavorably (neutral)',\n 'R2D2Opinion_Somewhat favorably',\n 'R2D2Opinion_Somewhat unfavorably',\n 'R2D2Opinion_Unfamiliar (N/A)',\n 'R2D2Opinion_Very favorably',\n 'R2D2Opinion_Very unfavorably',\n 'JarJarBinksOpinion_Neither favorably nor unfavorably (neutral)',\n 'JarJarBinksOpinion_Somewhat favorably',\n 'JarJarBinksOpinion_Somewhat unfavorably',\n 'JarJarBinksOpinion_Unfamiliar (N/A)',\n 'JarJarBinksOpinion_Very favorably',\n 'JarJarBinksOpinion_Very unfavorably',\n 'PadmeAmidalaOpinion_Neither favorably nor unfavorably (neutral)',\n 'PadmeAmidalaOpinion_Somewhat favorably',\n 'PadmeAmidalaOpinion_Somewhat unfavorably',\n 'PadmeAmidalaOpinion_Unfamiliar (N/A)',\n 'PadmeAmidalaOpinion_Very favorably',\n 'PadmeAmidalaOpinion_Very unfavorably',\n 'YodaOpinion_Neither favorably nor unfavorably (neutral)',\n 'YodaOpinion_Somewhat favorably',\n 'YodaOpinion_Somewhat unfavorably',\n 'YodaOpinion_Unfamiliar (N/A)',\n 'YodaOpinion_Very favorably',\n 'YodaOpinion_Very unfavorably',\n 'WhoShotFirst_Greedo',\n 'WhoShotFirst_Han',\n \"WhoShotFirst_I don't understand this question\",\n 'KnowsExpandedUniverse_No',\n 'KnowsExpandedUniverse_Yes',\n 'FanOfExpandedUniverse_No',\n 'FanOfExpandedUniverse_Yes',\n 'FanOfStarTrek_No',\n 'FanOfStarTrek_Yes',\n 'Gender_Female',\n 'Gender_Male',\n 'Location_East North Central',\n 'Location_East South Central',\n 'Location_Middle Atlantic',\n 'Location_Mountain',\n 'Location_New England',\n 'Location_Pacific',\n 'Location_South Atlantic',\n 'Location_West North Central',\n 'Location_West South Central']\n\n:::"
  },
  {
    "objectID": "Cleansing_Projects/project5.html#questiontask-3",
    "href": "Cleansing_Projects/project5.html#questiontask-3",
    "title": "Client Report - The war with the Star Wars",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article.\n\n\nShow the code\n#SeenFilm\nHasSeenMoviesPercent = [round(df_filtered['SeenFilm1'].count() / 835, 2) * 100, \n                        round(df_filtered['SeenFilm2'].count() / 835, 2) * 100, \n                        round(df_filtered['SeenFilm3'].count() / 835, 2) * 100, \n                        round(df_filtered['SeenFilm4'].count() / 835, 2) * 100, \n                        round(df_filtered['SeenFilm5'].count() / 835, 2) * 100, \n                        round(df_filtered['SeenFilm6'].count() / 835, 2) * 100]\n\nfig_1 = px.bar(x=HasSeenMoviesPercent,\n               y=['Star Wars: Episode I The Phantom Menace',\n                'Star Wars: Episode II Attack of the Clone',\n                'Star Wars: Episode III Revenge of the Sith',\n                'Star Wars: Episode IV A New Hope',\n                'Star Wars: Episode V The Empire Strikes Back',\n                'Star Wars: Episode VI Return of the Jedi'],\n                  text=HasSeenMoviesPercent,\n                  title=\"Which 'Star Wars' Movies Have You Seen? (Of 835 respondents)\",\n               )\nfig_1.show()\n\n\n# WhoShotFirst\nhanShotFirst = [round(df_filtered_ohe['WhoShotFirst_Han'].sum() / 834, 2) * 100,\n                round(df_filtered_ohe['WhoShotFirst_Greedo'].sum() / 834, 2) * 100,\n                round(df_filtered_ohe[\"WhoShotFirst_I don't understand this question\"].sum() / 834, 2) * 100]\n\nfig_2 = px.bar(x=hanShotFirst, \n               y=['Han', 'Greedo', 'I dont understand this question'],\n               title=\"Who Shot First? (According to 834 respondents)\",\n               text=hanShotFirst)\n\nfig_2.show()"
  },
  {
    "objectID": "Cleansing_Projects/project5.html#questiontask-4",
    "href": "Cleansing_Projects/project5.html#questiontask-4",
    "title": "Client Report - The war with the Star Wars",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.\nI used a DTC and a RFC to try to predict if a person taking the survey earns more than $50K. Unfortunately, the accuracy scores were not high enough to confirm that any of the model can predict this information. The accuracy score for the DTC is 0.60 or 60%, and the score for the RTC is 0.59 or 59%. These scores show that the models can predict right only that percentage of cases. Even though it is more than 50% and someone could say it can work, I also came to the conclusion that the data provided is not sufficently related with the income.\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nfeatures = df_filtered_ohe.columns\n\nfeatures = features.drop('Target')\nfeatures = features.drop('IncomeGroup')\n\nX = df_filtered_ohe[features]\ny = df_filtered_ohe['Target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Decision Tree Classifier\ndt_classifier = DecisionTreeClassifier()\n\ndt_classifier = dt_classifier.fit(X_train, y_train)\n\ny_pred = dt_classifier.predict(X_test)\n\ndt_classifier_accuracy = accuracy_score(y_test, y_pred)\n\n# Random Forest Classifier\nrf_classifier = RandomForestClassifier()\n\nrf_classifier.fit(X_train, y_train)\n\ny_pred_rf = rf_classifier.predict(X_test)\n\nrf_classifier_accuracy = accuracy_score(y_test, y_pred_rf)\n\n(dt_classifier_accuracy, rf_classifier_accuracy)\n\n\n(0.5444839857651246, 0.6014234875444839)"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Juan Zurita",
    "section": "",
    "text": "Computer Science Student at Brigham Young University - Idaho | Online TA for WDD 230\n\n&lt;a href=\"mailto:juanzurita@byui.edu\"&gt;juanzurita@byui.edu&lt;/a&gt; | \n&lt;a href=\"https://github.com/juan-zv\"&gt;My GitHub page&lt;/a&gt; | \n&lt;a href=\"https://www.linkedin.com/in/juanzuritavasquez/\"&gt;My LinkedIn page&lt;/a&gt;\n\n\n\nOnline Teacher Assistant for WDD 230\nRexburg, ID | Brigham Young University-Idaho Apr 2023-Present\n\nProvided guidance and support to 20+ students in web development using HTML, CSS, and JavaScript.\nGuided students to achieve proficiency in creating responsive, well-designed, and interactive web pages using HTML, CSS, and JavaScript.\nAssisted students in understanding and implementing web development concepts, troubleshoot coding issues, and contribute to a collaborative learning environment.\n\n\n\n\nBrigham Young University-Idaho\nRexburg, ID\nBachelor of Science Computer Science\nExpected Graduation Date: Apr 2026 -  Relevant Coursework: Web Frontend I, Data Science Programming, Applied Programming, etc. - OOP, Database handling, Optimization, Web Development. - GPA 3.78/4.0\nCertificate of Computer Programming Aug 2023 - Coursework: Algorithm Design, Data Structures, Object Oriented Programming, Work Methodology. - Emphasis in Software Development as a service while being part of a team.\n\n\n\nInternational Services Processor\nRexburg, ID | Brigham Young University-Idaho Nov 2022-Aug 2023\n\nEfficiently handled walk-ins, phone calls, and emails, providing information and directing students to appropriate departments.\nAssisted students with administrative processes and immigration-related questions, offering guidance and facilitating necessary paperwork.\nMaintained organized records, schedule appointments, and perform general administrative tasks with attention to detail and multitasking abilities.\n\n\n\nFull-time Volunteer Representative\nSanta Cruz, Bolivia | The Church of Jesus Christ of Latter-day Saints Jan 2020-Jan 2022\n\nTaught English as a second language for groups of 5-10 people.\nServed as an executive secretary, coordinating travel and logistics for volunteers.\nConducted training and goal-setting sessions, enhancing the effectiveness of volunteer efforts.\nLed and trained other volunteers as trainer, district leader, zone leader.\n\n\n\n\n\n\nJuan Zurita Portfolio: Static web page designed to showcase my projects and provide a professional bio.\nU&I Ride: Mobile app concept aimed at optimizing ride-sharing services in the Rexburg – Utah area.\nEstudio + Fe: Web application focused on enhancing scripture cross-referencing and note-taking capabilities for a wider audience.\n\n\n\n\n\n\n\nProficient in: Python, C#, JavaScript.\nFamiliar with: Dart, Kotlin, Swift, Rust. ### Web and Mobile Development\nExperienced in: SCSS, Firebase, Flutter, React, and React Native.\nFamiliar with: Android Studio and XCode. ### Tools and Technologies\nSkilled in SCRUM, Git, Web and Mobile layout design."
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Juan Zurita",
    "section": "",
    "text": "Online Teacher Assistant for WDD 230\nRexburg, ID | Brigham Young University-Idaho Apr 2023-Present\n\nProvided guidance and support to 20+ students in web development using HTML, CSS, and JavaScript.\nGuided students to achieve proficiency in creating responsive, well-designed, and interactive web pages using HTML, CSS, and JavaScript.\nAssisted students in understanding and implementing web development concepts, troubleshoot coding issues, and contribute to a collaborative learning environment."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Juan Zurita",
    "section": "",
    "text": "Brigham Young University-Idaho\nRexburg, ID\nBachelor of Science Computer Science\nExpected Graduation Date: Apr 2026 -  Relevant Coursework: Web Frontend I, Data Science Programming, Applied Programming, etc. - OOP, Database handling, Optimization, Web Development. - GPA 3.78/4.0\nCertificate of Computer Programming Aug 2023 - Coursework: Algorithm Design, Data Structures, Object Oriented Programming, Work Methodology. - Emphasis in Software Development as a service while being part of a team."
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Juan Zurita",
    "section": "",
    "text": "International Services Processor\nRexburg, ID | Brigham Young University-Idaho Nov 2022-Aug 2023\n\nEfficiently handled walk-ins, phone calls, and emails, providing information and directing students to appropriate departments.\nAssisted students with administrative processes and immigration-related questions, offering guidance and facilitating necessary paperwork.\nMaintained organized records, schedule appointments, and perform general administrative tasks with attention to detail and multitasking abilities.\n\n\n\nFull-time Volunteer Representative\nSanta Cruz, Bolivia | The Church of Jesus Christ of Latter-day Saints Jan 2020-Jan 2022\n\nTaught English as a second language for groups of 5-10 people.\nServed as an executive secretary, coordinating travel and logistics for volunteers.\nConducted training and goal-setting sessions, enhancing the effectiveness of volunteer efforts.\nLed and trained other volunteers as trainer, district leader, zone leader."
  },
  {
    "objectID": "resume.html#personal-projects",
    "href": "resume.html#personal-projects",
    "title": "Juan Zurita",
    "section": "",
    "text": "Juan Zurita Portfolio: Static web page designed to showcase my projects and provide a professional bio.\nU&I Ride: Mobile app concept aimed at optimizing ride-sharing services in the Rexburg – Utah area.\nEstudio + Fe: Web application focused on enhancing scripture cross-referencing and note-taking capabilities for a wider audience."
  },
  {
    "objectID": "resume.html#skills-and-interests",
    "href": "resume.html#skills-and-interests",
    "title": "Juan Zurita",
    "section": "",
    "text": "Proficient in: Python, C#, JavaScript.\nFamiliar with: Dart, Kotlin, Swift, Rust. ### Web and Mobile Development\nExperienced in: SCSS, Firebase, Flutter, React, and React Native.\nFamiliar with: Android Studio and XCode. ### Tools and Technologies\nSkilled in SCRUM, Git, Web and Mobile layout design."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Having a data science portfolio is essential for showing off your skills and experience in the field. It’s like a showcase of your real-world problem-solving abilities with data, making you stand out to potential employers, clients, or collaborators. It’s also a great way to track your progress and growth over time, helping you snag job opportunities and move forward in your data science career.\nThis is my portfolio which I want to share with anyone following my progress.\nMarkDown Basics Template Link to my repository"
  },
  {
    "objectID": "index.html#whis-is-it-important-to-have-a-portfolio",
    "href": "index.html#whis-is-it-important-to-have-a-portfolio",
    "title": "About Me",
    "section": "",
    "text": "Having a data science portfolio is essential for showing off your skills and experience in the field. It’s like a showcase of your real-world problem-solving abilities with data, making you stand out to potential employers, clients, or collaborators. It’s also a great way to track your progress and growth over time, helping you snag job opportunities and move forward in your data science career.\nThis is my portfolio which I want to share with anyone following my progress.\nMarkDown Basics Template Link to my repository"
  },
  {
    "objectID": "Cleansing_Projects/project1.html",
    "href": "Cleansing_Projects/project1.html",
    "title": "Client Report - What’s in a name?",
    "section": "",
    "text": "Analyzing the names_year.csv dataset using Pandas and Plotly Express reveals valuable insights into the evolution of names over time. By plotting the data, we can observe trends in naming preferences and popularity across different years. Key insights include identifying the emergence and decline of certain names, tracking the overall popularity of specific names over time, and potentially uncovering cultural or societal influences on naming trends. Additionally, visualizing the data allows the comparison of naming patterns between genders or regions, providing an understanding of how names have evolved throughout history.\n\n\nRead and format project data\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")"
  },
  {
    "objectID": "Cleansing_Projects/project1.html#elevator-pitch",
    "href": "Cleansing_Projects/project1.html#elevator-pitch",
    "title": "Client Report - What’s in a name?",
    "section": "",
    "text": "Analyzing the names_year.csv dataset using Pandas and Plotly Express reveals valuable insights into the evolution of names over time. By plotting the data, we can observe trends in naming preferences and popularity across different years. Key insights include identifying the emergence and decline of certain names, tracking the overall popularity of specific names over time, and potentially uncovering cultural or societal influences on naming trends. Additionally, visualizing the data allows the comparison of naming patterns between genders or regions, providing an understanding of how names have evolved throughout history.\n\n\nRead and format project data\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")"
  },
  {
    "objectID": "Cleansing_Projects/project1.html#questiontask-1",
    "href": "Cleansing_Projects/project1.html#questiontask-1",
    "title": "Client Report - What’s in a name?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nJuan is a name used often throughout the years. Its popularity was increasing and was definetiley pupular around the 2001 which is the year I was born. However, after the 2006, we can see a drop in the use of this name.\n\n\nRead and format data\nJuan = df.query(\"name == 'Juan'\")\n\nchart = px.line(Juan, x='year', y='Total', title='Juan Through Time')\n\nchart.add_vline(x='2001', line_dash=\"dash\", line_color=\"yellow\")\n\nchart.update_layout(xaxis_title='Year', yaxis_title='Total Count')\n\nchart.show()"
  },
  {
    "objectID": "Cleansing_Projects/project1.html#questiontask-2",
    "href": "Cleansing_Projects/project1.html#questiontask-2",
    "title": "Client Report - What’s in a name?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nMy first guess is that Brittany should eb young because that is just what I feel about that name. After analyzing I have realized that she could be between 36-24 years old. This chart shows how many people were named Brittany.\n\n\nRead and format data\n# Include and execute your code here\nBrittany = df.query(\"name == 'Brittany'\")\n\nchart2 = px.bar(Brittany, x='year', y='Total', title='Brittany Through Time')\n\nchart2.update_layout(xaxis_title='Year', yaxis_title='Total Count')\n\nchart2.show()"
  },
  {
    "objectID": "Cleansing_Projects/project1.html#questiontask-3",
    "href": "Cleansing_Projects/project1.html#questiontask-3",
    "title": "Client Report - What’s in a name?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice?\nFrom this analysis I realized that these names are being less used by everyone. Especially after the 50s. Culture is slowly leaving some christian customs behind and this is one of them.\n\n\nRead and format data\n# Include and execute your code here\n\nnames = (df\n        .query(\"name in ['Mary','Martha', 'Peter', 'Paul']\")\n        .query(\"year &gt;= 1920 and year &lt;=2000\"))\n\nnames_chart = px.line(names, x='year', y='Total', color = 'name', title='Mary, Martha, Peter and Paul Through Time')\n\nnames_chart.update_layout(xaxis_title='Year', yaxis_title='Total Count', legend_title=' ')\n\nnames_chart.show()"
  },
  {
    "objectID": "Cleansing_Projects/project1.html#questiontask-4",
    "href": "Cleansing_Projects/project1.html#questiontask-4",
    "title": "Client Report - What’s in a name?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nFor this question I chose “Forrest” from the movie “Forrest Gump” which came out in 1994. As we see, this movie really impacted society and I can say was a good hit because I really loved it. The movie definetiley had a big impact on the usage of the name and after that year, everyone stopped using it that much.\n\n\nRead and format data\n# Include and execute your code here\n\nForrest = df.query(\"name == 'Forrest'\")\n\nchart4 = px.line(Forrest, x='year', y='Total', title='Forrest Through Time')\n\nchart4.add_vline(x='1994', line_dash=\"dash\", line_color=\"yellow\")\n\nchart4.update_layout(xaxis_title='Year', yaxis_title='Total Count')\n\nchart4.show()"
  },
  {
    "objectID": "Cleansing_Projects/project2.html",
    "href": "Cleansing_Projects/project2.html",
    "title": "Client Report - Late flights and missing data (JSON files)",
    "section": "",
    "text": "Analyzing the flights_missing.json dataset offers insights into flight delays and associated factors such as airline performance and weather conditions. By exploring the data, we can uncover patterns in the frequency and duration of flight delays, identify which airlines are most prone to delays, and assess the impact of weather on flight schedules. Utilizing visualizations, we can depict correlations between delay durations and various parameters like time of day, day of the week, and weather conditions, enabling stakeholders to make informed decisions for improving flight operations and passenger experiences.\n\n\nRead and format project data\ndf = pd.read_json(\"https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json\")"
  },
  {
    "objectID": "Cleansing_Projects/project2.html#elevator-pitch",
    "href": "Cleansing_Projects/project2.html#elevator-pitch",
    "title": "Client Report - Late flights and missing data (JSON files)",
    "section": "",
    "text": "Analyzing the flights_missing.json dataset offers insights into flight delays and associated factors such as airline performance and weather conditions. By exploring the data, we can uncover patterns in the frequency and duration of flight delays, identify which airlines are most prone to delays, and assess the impact of weather on flight schedules. Utilizing visualizations, we can depict correlations between delay durations and various parameters like time of day, day of the week, and weather conditions, enabling stakeholders to make informed decisions for improving flight operations and passenger experiences.\n\n\nRead and format project data\ndf = pd.read_json(\"https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json\")"
  },
  {
    "objectID": "Cleansing_Projects/project2.html#questiontask-1",
    "href": "Cleansing_Projects/project2.html#questiontask-1",
    "title": "Client Report - Late flights and missing data (JSON files)",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the “NaN” for at least one missing value.\nI chose the 9th row which shows NaN on the key “minutes_delayed_nas”\n\n\nRead and format data\ndf.replace({\n    -999: np.nan,\n    'n/a': np.nan,\n    'Febuary': 'February',\n    '': np.nan,\n    '1500+': 1500\n}, inplace=True)\n\ndf = df.query(\"month != 'na'\")\n\navg_delay = df['num_of_delays_late_aircraft'].mean()\n\ndf['num_of_delays_late_aircraft'].replace(np.nan, avg_delay, inplace=True)\n\ndf.iloc[9]\n\n\nairport_code                                                                 IAD\nairport_name                     Washington, DC: Washington Dulles International\nmonth                                                                   February\nyear                                                                      2005.0\nnum_of_flights_total                                                       10042\nnum_of_delays_carrier                                                        284\nnum_of_delays_late_aircraft                                                631.0\nnum_of_delays_nas                                                            691\nnum_of_delays_security                                                         4\nnum_of_delays_weather                                                         28\nnum_of_delays_total                                                         1639\nminutes_delayed_carrier                                                  15573.0\nminutes_delayed_late_aircraft                                              39840\nminutes_delayed_nas                                                          NaN\nminutes_delayed_security                                                     169\nminutes_delayed_weather                                                     1359\nminutes_delayed_total                                                      78878\nName: 9, dtype: object"
  },
  {
    "objectID": "Cleansing_Projects/project2.html#questiontask-2",
    "href": "Cleansing_Projects/project2.html#questiontask-2",
    "title": "Client Report - Late flights and missing data (JSON files)",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays? Discuss the metric you chose, and why you chose it to determine the “worst” airport. Your answer should include a summary table that lists (for each airport) the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours.\nFor this question, I calculated the percentage of delayed flights each airport reported and sorted the values by the highest percentage of delayed fligths in each airport. This help us see which airport one should avoid because of the probability in 100 to have your flights delayed. It is the San Francisco (SFO) one\n\n\nRead and format data\nq2 = (df.groupby('airport_code').agg(\ntotal_flights=('num_of_flights_total', 'sum'),\ndelayed_flights=('num_of_delays_total', 'sum'),\navg_delay_time=('minutes_delayed_total', 'mean')))\n\nq2['delayed_percentage'] = q2['delayed_flights'] / q2['total_flights'] *100\n\nq2 = q2.sort_values(by='delayed_percentage', ascending=False)\n\nprint(q2)\n\n\n              total_flights  delayed_flights  avg_delay_time  \\\nairport_code                                                   \nSFO                 1630945           425604   201140.098485   \nORD                 3597588           830825   426940.371212   \nATL                 4430047           902443   408969.136364   \nIAD                  851571           168467    77905.136364   \nSAN                  917862           175132    62698.848485   \nDEN                 2513974           468519   190707.431818   \nSLC                 1403384           205160    76692.204545   \n\n              delayed_percentage  \nairport_code                      \nSFO                    26.095546  \nORD                    23.093945  \nATL                    20.370958  \nIAD                    19.783083  \nSAN                    19.080428  \nDEN                    18.636589  \nSLC                    14.618950"
  },
  {
    "objectID": "Cleansing_Projects/project2.html#questiontask-3",
    "href": "Cleansing_Projects/project2.html#questiontask-3",
    "title": "Client Report - Late flights and missing data (JSON files)",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length? Discuss the metric you chose and why you chose it to calculate your answer. Include one chart to help support your answer, with the x-axis ordered by month. (To answer this question, you will need to remove any rows that are missing the Month variable.)\nI dropped NaN values in month from the dataframe, and made a chart which shows the relationship between delays each month and total flights. This shows which month has the lowest percentage of delays which is September.\n\n\nRead and format data\ndf_months = df.dropna(subset=['month'])\n\nq3 = df.groupby('month').agg(\n    total_flights=('num_of_flights_total', 'sum'),\n    delayed_flights=('num_of_delays_total', 'sum')\n)\n\nq3['delayed_percentage'] = q3['delayed_flights'] / q3['total_flights']*100\n\n\nmonths_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\nq3 = q3.reindex(months_order)\n\nq3_visual = px.bar(q3, x=q3.index, y='delayed_percentage', \n                   color='delayed_percentage',\n                   labels={'delayed_percentage': 'Delayed Percentage'},\n                   title='Proportion of Delayed Flights by Month')\nq3_visual.update_layout(xaxis_title='Month', yaxis_title='Proportion of Delayed Flights')\n\nq3_visual.show()"
  },
  {
    "objectID": "Cleansing_Projects/project2.html#questiontask-4",
    "href": "Cleansing_Projects/project2.html#questiontask-4",
    "title": "Client Report - Late flights and missing data (JSON files)",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. Your job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). You will need to replace all the missing values in the Late Aircraft variable with the mean. Show your work by printing the first 5 rows of data in a table. Use these three rules for your calculations:\nI successfully included both categories into one column and used the conditions to make the calculations to create that column.\n\n\nRead and format data\ndelays_mean = df['num_of_delays_late_aircraft'].mean(skipna=True)\n\ndf['num_of_delays_late_aircraft'].fillna(delays_mean, inplace=True)\n\n\n100% of delayed flights in the Weather category are due to weather\n\n\nRead and format data\ndf['num_of_delays_weather_total'] = df['num_of_delays_weather']\n\n\n30% of all delayed flights in the Late-Arriving category are due to weather.\n\n\nRead and format data\ndf['num_of_delays_weather_total'] += df['num_of_delays_late_aircraft'] * 0.3\n\n\nFrom April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.\n\n\nRead and format data\ndf.loc[df['month'].isin(['April', 'May', 'June', 'July', 'August']), 'num_of_delays_weather_total'] += df['num_of_delays_nas'] * 0.4\ndf.loc[~df['month'].isin(['April', 'May', 'June', 'July', 'August']), 'num_of_delays_weather_total'] += df['num_of_delays_nas'] * 0.65\n\nprint(df.head())\n\n\n  airport_code                                       airport_name    month  \\\n0          ATL  Atlanta, GA: Hartsfield-Jackson Atlanta Intern...  January   \n1          DEN                   Denver, CO: Denver International  January   \n2          IAD                                                NaN  January   \n3          ORD          Chicago, IL: Chicago O'Hare International  January   \n4          SAN             San Diego, CA: San Diego International  January   \n\n     year  num_of_flights_total num_of_delays_carrier  \\\n0  2005.0                 35048                  1500   \n1  2005.0                 12687                  1041   \n2  2005.0                 12381                   414   \n3  2005.0                 28194                  1197   \n4  2005.0                  7283                   572   \n\n   num_of_delays_late_aircraft  num_of_delays_nas  num_of_delays_security  \\\n0                  1109.104072               4598                      10   \n1                   928.000000                935                      11   \n2                  1058.000000                895                       4   \n3                  2255.000000               5415                       5   \n4                   680.000000                638                       7   \n\n   num_of_delays_weather  num_of_delays_total  minutes_delayed_carrier  \\\n0                    448                 8355                 116423.0   \n1                    233                 3153                  53537.0   \n2                     61                 2430                      NaN   \n3                    306                 9178                  88691.0   \n4                     56                 1952                  27436.0   \n\n   minutes_delayed_late_aircraft  minutes_delayed_nas  \\\n0                         104415             207467.0   \n1                          70301              36817.0   \n2                          70919              35660.0   \n3                         160811             364382.0   \n4                          38445              21127.0   \n\n   minutes_delayed_security  minutes_delayed_weather  minutes_delayed_total  \\\n0                       297                    36931                 465533   \n1                       363                    21779                 182797   \n2                       208                     4497                 134881   \n3                       151                    24859                 638894   \n4                       218                     4326                  91552   \n\n   num_of_delays_weather_total  \n0                  3769.431222  \n1                  1119.150000  \n2                   960.150000  \n3                  4502.250000  \n4                   674.700000"
  },
  {
    "objectID": "Cleansing_Projects/project2.html#questiontask-5",
    "href": "Cleansing_Projects/project2.html#questiontask-5",
    "title": "Client Report - Late flights and missing data (JSON files)",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.\nFrom this barplot we see that there is a high proportion caused by weather in general. If we only used the weather condition provided, we would probably see less of a proportion thus making us feel like there is not a big relationship between common weather delays and total delays.\n\n\nRead and format data\ndf['total_weather_delays'] = df['num_of_delays_nas'] + df['num_of_delays_weather']\n\ntotal_flights = df.groupby('airport_code')['num_of_flights_total'].sum().reset_index()\n\nq5 = df.merge(total_flights, on='airport_code', how='left')\nq5['all_delayed_by_weather'] = (q5['total_weather_delays'] / total_flights['num_of_flights_total'])*100\n\n\nq5_fig = px.bar(q5, x='airport_code', y='all_delayed_by_weather',\ntitle='Proportion of Flights Delayed by Weather at Each Airport',\nlabels={'weather_delay_proportion': 'Proportion of Flights Delayed by Weather', 'airport': 'Airport'})\nq5_fig.update_xaxes(title_text='Airport')\nq5_fig.update_yaxes(title_text='Proportion of Flights Delayed by Weather')\n\nq5_fig.show()"
  },
  {
    "objectID": "Cleansing_Projects/project3.html",
    "href": "Cleansing_Projects/project3.html",
    "title": "Client Report - Finding relationships in baseball",
    "section": "",
    "text": "Show the code\n#Establish connection with database\nsqlite_file = r'lahmansbaseballdb.sqlite'\n\ncon = sqlite3.connect(sqlite_file)\n\nq = '''\n    SELECT * \n    FROM sqlite_master \n    WHERE type='table'\n    '''\n    \ntable = pd.read_sql_query(q,con)\ntable.filter(['name'])\n\n\n\n\n\n\n\n\n\nname\n\n\n\n\n0\nallstarfull\n\n\n1\nappearances\n\n\n2\nawardsmanagers\n\n\n3\nawardsplayers\n\n\n4\nawardssharemanagers\n\n\n5\nawardsshareplayers\n\n\n6\nbatting\n\n\n7\nbattingpost\n\n\n8\ncollegeplaying\n\n\n9\ndivisions\n\n\n10\nfielding\n\n\n11\nfieldingof\n\n\n12\nfieldingofsplit\n\n\n13\nfieldingpost\n\n\n14\nhalloffame\n\n\n15\nhomegames\n\n\n16\nleagues\n\n\n17\nmanagers\n\n\n18\nmanagershalf\n\n\n19\nparks\n\n\n20\npeople\n\n\n21\npitching\n\n\n22\npitchingpost\n\n\n23\nsalaries\n\n\n24\nschools\n\n\n25\nseriespost\n\n\n26\nteams\n\n\n27\nteamsfranchises\n\n\n28\nteamshalf"
  },
  {
    "objectID": "Cleansing_Projects/project3.html#elevator-pitch",
    "href": "Cleansing_Projects/project3.html#elevator-pitch",
    "title": "Client Report - Finding relationships in baseball",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nAnalyzing the “lahmansbaseball.db.sqlite” database using SQLite and Python’s sqlite3 library provides valuable insights into various aspects of baseball, including team performance, player statistics, and league dynamics. By querying the database, we can uncover trends in team standings over multiple seasons, track individual player performances, and assess the overall competitiveness of different leagues. Visualizing the data can help in understanding patterns such as batting averages, home run frequencies, and pitching statistics, allowing analysts and enthusiasts to gain deeper insights into the sport’s dynamics and evolution over time. Additionally, exploring relationships between player demographics, team strategies, and game outcomes can offer valuable insights for teams, coaches, and fans alike."
  },
  {
    "objectID": "Cleansing_Projects/project3.html#questiontask-1",
    "href": "Cleansing_Projects/project3.html#questiontask-1",
    "title": "Client Report - Finding relationships in baseball",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThis table shows information for players with school ID as idbyuid and their salary with the year and the team they were playing for. \n\n\nRead and format data\nq1 = '''\n    SELECT DISTINCT p.playerID, s.schoolID, sa.salary, sa.yearID, sa.teamID\n    FROM people AS p\n    INNER JOIN collegeplaying AS c ON p.playerID = c.playerID\n    INNER JOIN schools AS s ON c.schoolID = s.schoolID\n    INNER JOIN salaries AS sa ON p.playerID = sa.playerID\n    WHERE s.schoolID = 'idbyuid'\n    ORDER BY sa.salary DESC;\n    '''\n    \nquestion_1 = pd.read_sql_query(q1,con)\n    \nquestion_1\n\n\n\n\n\n\n\n\n\nplayerID\nschoolID\nsalary\nyearID\nteamID\n\n\n\n\n0\nlindsma01\nidbyuid\n4000000.0\n2014\nCHA\n\n\n1\nlindsma01\nidbyuid\n3600000.0\n2012\nBAL\n\n\n2\nlindsma01\nidbyuid\n2800000.0\n2011\nCOL\n\n\n3\nlindsma01\nidbyuid\n2300000.0\n2013\nCHA\n\n\n4\nlindsma01\nidbyuid\n1625000.0\n2010\nHOU\n\n\n5\nstephga01\nidbyuid\n1025000.0\n2001\nSLN\n\n\n6\nstephga01\nidbyuid\n900000.0\n2002\nSLN\n\n\n7\nstephga01\nidbyuid\n800000.0\n2003\nSLN\n\n\n8\nstephga01\nidbyuid\n550000.0\n2000\nSLN\n\n\n9\nlindsma01\nidbyuid\n410000.0\n2009\nFLO\n\n\n10\nlindsma01\nidbyuid\n395000.0\n2008\nFLO\n\n\n11\nlindsma01\nidbyuid\n380000.0\n2007\nFLO\n\n\n12\nstephga01\nidbyuid\n215000.0\n1999\nSLN\n\n\n13\nstephga01\nidbyuid\n185000.0\n1998\nPHI\n\n\n14\nstephga01\nidbyuid\n150000.0\n1997\nPHI"
  },
  {
    "objectID": "Cleansing_Projects/project3.html#questiontask-2",
    "href": "Cleansing_Projects/project3.html#questiontask-2",
    "title": "Client Report - Finding relationships in baseball",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\nWrite an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\n\n\nRead and format data\nq2a = '''\n   SELECT playerID, yearID, \n   ROUND(CAST(SUM(H) AS FLOAT) / NULLIF(SUM(AB), 0), 3) AS batting_average\n    FROM batting\n    WHERE AB &gt; 0\n    GROUP BY playerID, yearID\n    ORDER BY batting_average DESC\n    LIMIT 5;\n    '''\nquestion_2a = pd.read_sql_query(q2a,con)\n    \nquestion_2a\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nbatting_average\n\n\n\n\n0\nabernte02\n1960\n1.0\n\n\n1\nabramge01\n1923\n1.0\n\n\n2\nacklefr01\n1964\n1.0\n\n\n3\nalanirj01\n2019\n1.0\n\n\n4\nalberan01\n2017\n1.0\n\n\n\n\n\n\n\nUse the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\n\n\nRead and format data\nq2b = '''\n    SELECT playerID, yearID, \n       ROUND(CAST(SUM(H) AS FLOAT) / NULLIF(SUM(AB), 0), 3) AS batting_average\n    FROM batting\n    WHERE AB &gt;= 10  \n    GROUP BY playerID, yearID\n    ORDER BY batting_average DESC, playerID\n    LIMIT 5;\n'''\nquestion_2b = pd.read_sql_query(q2b,con)\n    \nquestion_2b\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nbatting_average\n\n\n\n\n0\nnymanny01\n1974\n0.643\n\n\n1\ncarsoma01\n2013\n0.636\n\n\n2\naltizda01\n1910\n0.600\n\n\n3\nsilvech01\n1948\n0.571\n\n\n4\npuccige01\n1930\n0.563\n\n\n\n\n\n\n\nNow calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\n\n\nRead and format data\nq2c = '''\n    SELECT playerID, \n       ROUND(CAST(SUM(H) AS FLOAT) / NULLIF(SUM(AB), 0), 3) AS career_batting_average\n    FROM batting\n    GROUP BY playerID\n    HAVING SUM(AB) &gt;= 100  \n    ORDER BY career_batting_average DESC\n    LIMIT 5;\n'''\nquestion_2c = pd.read_sql_query(q2c,con)\n    \nquestion_2c\n\n\n\n\n\n\n\n\n\nplayerID\ncareer_batting_average\n\n\n\n\n0\ncobbty01\n0.366\n\n\n1\nbarnero01\n0.360\n\n\n2\nhornsro01\n0.358\n\n\n3\njacksjo01\n0.356\n\n\n4\nmeyerle01\n0.356"
  },
  {
    "objectID": "Cleansing_Projects/project3.html#questiontask-3",
    "href": "Cleansing_Projects/project3.html#questiontask-3",
    "title": "Client Report - Finding relationships in baseball",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Plotly Express to visualize the comparison. What do you learn?\nFor this question I chose the Seattle Mariners and the Boston Reds to compare their team’s total salary. I see that the total salary from the Boston Reds is higher than the Mariners which makes me think they are better paid, making any baseball player to aim to play for that team. I also think about each city’s economy over time and in general which may also explain why Boston players would have to be paid more than Seattle ones.\n\n\nRead and format data\nq3 = '''\n    SELECT teamID, SUM(salary) AS salary\n    FROM salaries\n    WHERE teamID IN ('SEA', 'BOS')\n    GROUP BY teamID;\n'''\n\nquestion_3 = pd.read_sql_query(q3,con)\n    \nquestion_3\n\n\nvisual = px.bar(question_3, x='teamID', y='salary', title='Salaries Comparison',\n             labels={'teamID': 'Team', 'salary': 'Team Total Salary'})\n\nvisual.show()"
  }
]